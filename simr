#!/bin/bash

#############################################################
# Set this if the hadoop binary/script is not in the PATH   #
# HADOOP=/root/hadoop/bin/hadoop                            #
#############################################################

SIMRJAR=$(eval "ls -1 simr*.jar | head -1")

#############################################################
# Set SIMRJAR to point to the desired SIMR jar distribution #
# Otherwise, it picks the first simr*.jar file found        #
# SIMRJAR=simr-cdh3.jar                                     #
#############################################################

INPATH=$(type -P "hadoop") 
PROGNAME=$(basename $0)


if [ -f "$INPATH" ]; then
  HADOOPBIN=$INPATH
elif [ -n "$HADOOP" -a -f "$HADOOP"  ]; then
  HADOOPBIN=$HADOOP
elif [ -n "$HADOOP" -a -f "$HADOOP/bin/hadoop" ]; then
  HADOOPBIN="$HADOOP/bin/hadoop"
elif [ -n "$HADOOP" -a -f "$HADOOP/hadoop" ]; then
  HADOOPBIN="$HADOOP/hadoop"
fi

if [ ! -f "$HADOOPBIN" ];
then
   echo -e "Error:\n  Couldn't find the hadoop executable, add it to the PATH or point \$HADOOP to it."
   exit 1
fi

if [ "$#" -le 0 ];
then
  echo "Usage:"
  echo "  $PROGNAME [--slots=<num>] [--memory=<size>] [--outdir=<hdfs_out_dir>] --shell"
  echo "    Runs an interactive Spark Shell inside a MapReduce job hon a Hadoop cluster."
  echo ""
  echo "  $PROGNAME [--slots=<num>] [--memory=<size>] [--outdir=<hdfs_out_dir>] <your_spark_jar> <main_class_name> <parameters>"
  echo "    Runs your Spark job that is inside your provided jar-file with supplied parameters."
  echo ""
  echo "    --slots=<num> can optionally be set to acquire <num> number of map slots in the cluster."
  echo ""
  echo "    --memory=<num> can optionally be set to specify the amount of memory for each executor to be <num> MB"
  echo "      Example: --memory=1024 will set the heap size of each executor to 1024 MB."
  echo ""
  echo "    --outdir=<hdfs_out_dir> can optionally be specified to store the output of the job in <hdfs_out_dir> in HDFS"
  echo ""
  echo "    <parameters> will be passed to your <main_class_name>, with %spark_url%"
  echo "      replaced with the Spark URL that must be passed to SparkContext."
  echo ""
  echo "Examples:"
  echo "  $PROGNAME --slots=2 --shell"
  echo ""
  echo "  $PROGNAME --slots=10 spark-examples.jar org.apache.spark.examples.SparkPi %master_url% 9"
  echo ""
  exit 1
fi

USERJAR=$1
MAINCLASS=$2

if [ -a "$USERJAR" ];
then
  USERJAR=$(readlink -e $USERJAR)
fi

shift
shift

cd "`dirname $0`"

if [ ! -f "$SIMRJAR" ];
then
  echo -e "Error:\n " \
          "  Couldn't locate $SIMRJAR, make sure it exists in the same directory as this script"
  exit 1
fi

fsize=$(ls -1hl $SIMRJAR | cut -d " " -f 5)
echo "Uploading $SIMRJAR ($fsize) to the Hadoop cluster, this may take a while"

HADOOP_USER_CLASSPATH_FIRST=1 HADOOP_CLASSPATH=$SIMRJAR exec $HADOOPBIN jar $SIMRJAR $USERJAR $MAINCLASS $@
